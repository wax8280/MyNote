# 线性回归

我们看到一个例子：

![](./static/stanford_ml_note1_part1_1.JPG)

在这里假设$x$是一个在$R^{2}$里面的两位向量。比如，$x_{1}^{i}$是训练集里面的Living area。

为了执行监督学习，我们设定一个函数$h$，我们称之为假设函数（hypotheses）。我们先假设一个线性函数：

$h_{\theta}(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}$

在这里$\theta_{i}$是一个参数（也称为权重），代表函数从X到Y映射的线性函数的空间。

我们令$x_{0}=1$，则有：

$h(x)=\sum_{n}^{i=0}\theta_{i}x_{i}=\theta^{T}x$

$n$代表训练集输入的属性维度数。

我们假设$y$是正确的结果则我们希望，$\frac{1}{2}\sum_{i=1}^{m}(h_{\theta}(x)-y)^{2}$尽可能小。所以我们定义

$J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(h_{\theta}(x)-y)^{2}$

## LMS算法

我们当然希望找到最小的$J(\theta)$，所以我们使用一些搜索算算法。在这里我们梯度下降（gradient descent）算法。



## 矩阵求导

假设$f$函数是一个以$R^{m*n}$（m*n矩阵）输入，实数输出的函数。

