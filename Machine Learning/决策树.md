# 决策树

* 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
* 缺点：可能会产生过度匹配问题。
* 适用数据类型：数值型和标称型。

决策树的一般流程

* 收集数据：可以使用任何方法。
* 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。
* 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
* 训练算法：构造树的数据结构。
* 测试算法：使用经验树计算错误率。
* 使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。 

## 信息增益

数据特征的划分过程是一个将数据集从无序变为有序的过程。如果每次都选出特征集中无序度最大的那列特征作为划分节点？我们由香农的《信息论》知道，信息是对准确定的消除。换而言之，事件出现的概率越大，信息量越小。

信息量：$l(x_{i})=-log_{2}p(x_{i})$

若信源事件有n种取值：信源的平均不确定性应为单个符号不确定性的统计平均值，其中$p(x_{i})$为样本发生的概率。

信息熵：$H=-\sum_{i=1}^{n}p(x_{i})log_{2}p(x_{i})$。

信息期望：$E(A)=-\sum_{j=1}^{n}\frac{s_{1j}+s_{2j}+...+s_{mj}}{s}I(s_{1j},s_{2j},...,s_{mj})$

其中$\frac{s_{1j}+s_{2j}+...+s_{mj}}{s}$是第$j$个子集的权，并且等于子集中样本个数除以$S$中的样本总数。

信息增益是决策树某个分支上整个数据集信息熵与当前节点信息熵的差值。

信息增益：$Gain(A)=I(s_{1},s_{2},...,s_{m})-E(A)$

## ID3决策树

假设有个销售调查表

| 计数   | 年龄   | 收入   | 学生   | 信誉   | 是否购买 |
| ---- | ---- | ---- | ---- | ---- | ---- |
| 64   | 0    | 2    | 0    | 0    | 0    |
| 64   | 0    | 2    | 0    | 1    | 0    |
| 128  | 1    | 2    | 0    | 0    | 1    |
| 60   | 2    | 1    | 0    | 0    | 1    |
| 64   | 2    | 0    | 1    | 0    | 1    |
| 64   | 2    | 0    | 1    | 1    | 0    |
| 64   | 1    | 0    | 1    | 1    | 1    |
| 128  | 0    | 1    | 0    | 0    | 0    |
| 64   | 0    | 0    | 1    | 0    | 1    |
| 132  | 2    | 1    | 1    | 0    | 1    |
| 64   | 0    | 1    | 1    | 1    | 1    |
| 32   | 1    | 1    | 0    | 1    | 1    |
| 32   | 1    | 2    | 1    | 0    | 1    |
| 64   | 2    | 1    | 0    | 1    | 0    |

### ID3算法

ID3算法通过计算所有特征的信息增益，而选出划分点。（详细计算例子请参考《机器学习，算法原理与编程实践》P99）


```python
from math import log
import operator

# 年龄 收入 学生 信誉 是否购买
d = [[0, 2, 0, 0, 0],
     [0, 2, 0, 1, 0],
     [1, 2, 0, 0, 1],
     [2, 1, 0, 0, 1],
     [2, 0, 1, 0, 1],
     [2, 0, 1, 1, 0],
     [1, 0, 1, 1, 1],
     [0, 1, 0, 0, 0],
     [0, 0, 1, 0, 1],
     [2, 1, 1, 0, 1],
     [0, 1, 1, 1, 1],
     [1, 1, 0, 1, 1],
     [1, 2, 1, 0, 1],
     [2, 1, 0, 1, 0]]

# 数量
count = [64, 64, 128, 60, 64, 64, 64, 128, 64, 132, 64, 32, 32, 64]


def createDataSet():
    dataSet = []
    for index, i in enumerate(count):
        dataSet.extend([d[index] for j in range(i)])
    labels = ['years', 'salary', 'student', 'credit']
    # change to discrete values
    return dataSet, labels


def calcShannonEnt(dataSet):
    """计算信息熵"""
    numEntries = len(dataSet)
    labelCounts = {}
    for featVec in dataSet:  # the the number of unique elements and their occurance
        # 从数据集中得到类别标签
        currentLabel = featVec[-1]
        if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0
        labelCounts[currentLabel] += 1
    shannonEnt = 0.0
    for key in labelCounts:
        prob = float(labelCounts[key]) / numEntries
        shannonEnt -= prob * log(prob, 2)  # log base 2
    return shannonEnt


def splitDataSet(dataSet, axis, value):
    """
    划分数据集，删除特征轴所在的数据列返回剩余的数据集
    :param dataSet: 数据集
    :param axis: 特征轴
    :param value: 特征轴的取值
    :return:
    """
    retDataSet = []
    for featVec in dataSet:
        if featVec[axis] == value:
            reducedFeatVec = featVec[:axis]  # chop out axis used for splitting
            reducedFeatVec.extend(featVec[axis + 1:])
            retDataSet.append(reducedFeatVec)
    return retDataSet


def chooseBestFeatureToSplit(dataSet):
    """计算最优特征"""
    # 计算特征向量维度，其中最一列用于类别标签，减去
    numFeatures = len(dataSet[0]) - 1
    # 计算信息熵
    baseEntropy = calcShannonEnt(dataSet)

    # 初始化最优的信息增益
    bestInfoGain = 0.0
    # 初始化最优的特征轴
    bestFeature = -1

    # 遍历所有特征
    for i in range(numFeatures):
        # 所有数据集第i个特征值
        featList = [example[i] for example in dataSet]
        # 创建唯一的特征值列表
        uniqueVals = set(featList)
        # 初始化该列的信息熵
        newEntropy = 0.0

        # 对每个特征划分一次数据集，然后计算数据集的新熵值，并对所有唯一特征值得到的熵求和。
        for value in uniqueVals:
            subDataSet = splitDataSet(dataSet, i, value)
            # 权值
            prob = len(subDataSet) / float(len(dataSet))
            # 信息期望
            newEntropy += prob * calcShannonEnt(subDataSet)

        # 计算最大增益
        infoGain = baseEntropy - newEntropy
        if (infoGain > bestInfoGain):
            bestInfoGain = infoGain
            bestFeature = i
    return bestFeature


if __name__ == '__main__':
    myDat, labels = createDataSet()
    print(chooseBestFeatureToSplit(myDat))
```

通过以上代码，可以求得“年龄”特征的信息增益最大。

目前我们已经学习了从数据集构造决策树算法所需要的子功能模块，其工作原理如下：得到原始数据集，然后基于最好的特征值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个节点上 ，我们可以再次划分数据。因此我们可以采用递归的原则处理数据集。 

递归结束的条件是：程序遍历完所有划分数据集的特征，或者每个分支下的所有实例都具有相同的分类。如果所有实例具有相同的分类，则得到一个叶子节点或者终止块。任何到达叶子节点的数据必然属于叶子节点的分类 。

第一个结束条件使得算法可以终止，我们甚至可以设置算法可以划分的最大分组数目。后续章节还会介绍其他决策树算法，如C4.5和CART ，这些算法在运行时并不总是在每次划分分组时都会消耗特征。由于特征数目并不是在每次划分数据分组时都减少，因此这些算法在实际使用时可能引起一定的问题。目前我们并不需要考虑这个问题，只需要在算法开始运行前计算列的数目，查看算法是否使用了所有特征即可。 

```python
def majorityCnt(classList):
    """返回出现次数最多的标签"""
    count = [(classList.count(i), i) for i in set(classList)]
    return max(count)[1]


def createTree(dataSet, labels):
    # 忽略元数据的决策树标签列
    classList = [example[-1] for example in dataSet]

    # 递归函数的第一个停止条件是所有的类标签完全相同，则直接返回该类标签
    # 程序终止条件1：如果classList只有一种决策标签，停止划分，返回这个决策标签
    if classList.count(classList[0]) == len(classList):
        return classList[0]
    # 递归函数的第二个停止条件是使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分
    # 程序终止条件2：如果数据集的第一个决策标签只有一个，则返回这个标签
    # 由于第二个条件无法简单地返回唯一的类标签 ，这里使用majorityCnt函数挑选出现次数最多的类别作为返回值。
    if len(dataSet[0]) == 1:
        return majorityCnt(classList)

    bestFeat = chooseBestFeatureToSplit(dataSet)
    bestFeatLabel = labels[bestFeat]
    myTree = {bestFeatLabel: {}}
    del (labels[bestFeat])
    uniqueVals = set([example[bestFeat] for example in dataSet])
    for value in uniqueVals:
        # 复制
        subLabels = labels[:]
        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)
    return myTree


if __name__ == '__main__':
    myDat, labels = createDataSet()
    print(createTree(myDat, labels))
```

最后我们可以得到结果

```
{'years': {0: {'student': {0: 0, 1: 1}}, 1: 1, 2: {'credit': {0: 1, 1: 0}}}}
```



